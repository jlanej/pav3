# NOTE: AI-generated workflow. Review before use.
#
# Common pitfalls:
#   - The Docker build stage downloads samtools, minimap2, LRA, and bedToBigBed
#     from external URLs. If any download host is unreachable, the build will
#     fail. Consider caching the build_deps stage or pre-building a base image.
#   - GitHub Actions runners have ~14 GB of disk and ~7 GB of RAM. The pav3
#     pipeline may hit memory limits on larger test datasets; the current ~2 Mb
#     test region is sized to stay well within these constraints.
#   - The integration test can take 20+ minutes due to the Docker build (compiling
#     samtools, minimap2, LRA from source). Docker layer caching helps on re-runs
#     but the first run on a fresh runner will be slow.
#   - If the test data FASTA files change, their .fai/.gzi index files must be
#     regenerated. A mismatch will cause samtools/pysam errors inside the container.

name: Docker Integration Test

on:
  push:
    branches: [main, dev]
    paths:
      - 'Dockerfile'
      - 'files/docker/**'
      - 'src/**'
      - 'pyproject.toml'
      - 'tests/**'
  pull_request:
    branches: [main]
    paths:
      - 'Dockerfile'
      - 'files/docker/**'
      - 'src/**'
      - 'pyproject.toml'
      - 'tests/**'
  workflow_dispatch:

jobs:
  docker-integration:
    name: Build & Run pav3 Docker
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Build Docker image
        run: docker build -t pav3-test .

      - name: Verify pav3 installation
        run: docker run --rm pav3-test --version

      - name: Stage test data
        run: |
          WORK_DIR=$(mktemp -d)
          echo "WORK_DIR=${WORK_DIR}" >> "$GITHUB_ENV"

          cp -r tests/test_data/ref "${WORK_DIR}/"
          cp -r tests/test_data/assemblies "${WORK_DIR}/"
          cp tests/test_data/pav.json "${WORK_DIR}/"
          cp tests/test_data/assemblies.tsv "${WORK_DIR}/"

      - name: Run pav3 batch
        run: |
          docker run --rm \
            -v "${WORK_DIR}:${WORK_DIR}" \
            --user "$(id -u):$(id -g)" \
            --workdir "${WORK_DIR}" \
            pav3-test \
            batch --cores 4 --keep-going

      - name: Validate output
        run: |
          echo "=== Output files ==="
          find "${WORK_DIR}" -type f | sort

          echo ""
          echo "=== VCF files ==="
          VCF_FILES=$(find "${WORK_DIR}" -name "*.vcf.gz" || true)
          if [ -z "${VCF_FILES}" ]; then
            echo "WARNING: No VCF files produced"
          else
            echo "${VCF_FILES}"
          fi

      - name: Generate summary report
        if: always()
        run: |
          echo "PAV3 Integration Test Report"
          echo "============================"
          echo "Date: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo ""

          echo "Output file count: $(find "${WORK_DIR}" -type f | wc -l)"
          echo ""

          VCF_FILES=$(find "${WORK_DIR}" -name "*.vcf.gz" 2>/dev/null || true)
          for vcf in ${VCF_FILES}; do
            echo "--- $(basename "${vcf}") ---"
            total=$(zgrep -cv '^#' "${vcf}" 2>/dev/null || echo 0)
            echo "  Total records: ${total}"
            snv=$(zgrep -v '^#' "${vcf}" 2>/dev/null | grep -c 'SVTYPE=SNV' || echo 0)
            ins=$(zgrep -v '^#' "${vcf}" 2>/dev/null | grep -c 'SVTYPE=INS' || echo 0)
            del=$(zgrep -v '^#' "${vcf}" 2>/dev/null | grep -c 'SVTYPE=DEL' || echo 0)
            inv=$(zgrep -v '^#' "${vcf}" 2>/dev/null | grep -c 'SVTYPE=INV' || echo 0)
            echo "  SNVs: ${snv}  INS: ${ins}  DEL: ${del}  INV: ${inv}"
          done

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pav3-integration-results
          path: ${{ env.WORK_DIR }}
          retention-days: 7
          if-no-files-found: warn
